{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning\n",
    "\n",
    "Training set: $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$\n",
    "\n",
    "Unsupervised learning\n",
    "\n",
    "Training set: $x^{(1)},x^{(2)},x^{(3)},...,x^{(m)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 K‐means algorithm\n",
    "\n",
    "### 7.1.1 Input:\n",
    "\n",
    "- K (Number of clusters)\n",
    "\n",
    "- Training set $x^{(1)},x^{(2)},x^{(3)},...,x^{(m)}$\n",
    "\n",
    "$x^{(i)}∈R^n$ (Drop $x_0 = 1$ converntion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Method:\n",
    "\n",
    "随机初始化K个μ, 其中每个μ都含有n个维度。\n",
    "\n",
    "$μ_1,μ_2,μ_3,μ_4,...,μ_k∈R^n$\n",
    "\n",
    "重复：\n",
    "\n",
    "for i = 1 to m 对于每个数据\n",
    "\n",
    "{\n",
    "\n",
    "$c^{(i)} ：= x^{(i)} 被划分到簇的下标$\n",
    "        \n",
    "$比如，被划分到第一簇，c=1，第二簇，c=2$\n",
    "\n",
    "}\n",
    "\n",
    "for k = 1 to K 对于每个簇\n",
    "\n",
    "{\n",
    "\n",
    "$μ_k ：= 每个属于此簇的x的值的平均数$\n",
    "\n",
    "$μ_k ∈ R^n 由于x为n维，μ也是n维$\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 Cost Function:\n",
    "\n",
    "K‐means optimization objective\n",
    "\n",
    "$c^{(i)}:$ index of cluster (1,2,…,) to which example is currently assigned\n",
    "\n",
    "$μ_{k}:$ cluster centroid k $μ_k ∈ R^n$\n",
    "\n",
    "$μ_{c^{(i)}}:$cluster centroid of cluster to which example has been assigned\n",
    "\n",
    "$J(c^{(1)},...,c^{(m)},μ_{(1)},μ_{(k)}) = \\frac{1}{m} \\sum_{i=1}^m ||x^{(i)}-μ_{c^{(i)}}||^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Fuction即最小化J函数得到min的 $c^{(1)},c^{(2)},...,c^{(m)}$ 以及 $μ_1,μ_2..,μ_k$, 其中c代表点的簇，u代表此簇的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4 Random initialization\n",
    "\n",
    "确保 K < m，随机从训练数据中选出K个点，并且将这些点赋值给 $μ_1,μ_2,μ_3,μ_4,...,μ_k∈R^n$\n",
    "\n",
    "由于Cluster容易陷入局部最优，所以需要多次初始化值并且进行多次聚类。具体来说：\n",
    "\n",
    "设置一个100次的循环，然后随机选择初始值，求 J，选择具有最小 cost J 的那次循环和相应的 k 个类别的中心点 μ 以及所有输入参数的类别 c 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.5 Choosing the number of clusters\n",
    "\n",
    "一种方法称之为Elbow method，即分别从k = 1， 2， 3， 4开始绘制k-J曲线，找到明显拐点，但是，其实很多时候下降都是平缓的，不存在拐点。\n",
    "\n",
    "**注意，如果k-J函数不是一直下降，那么必然在某处陷入了局部最优。对于此k值，在上一步加大循环次数获取合适的J，设置为此处的J，结果一定是递减的。**\n",
    "\n",
    "此外，我们有时还可以根据需求选择k，比如衣服的M,L,XL码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Motivation\n",
    "\n",
    "### 7.2.1 Data Compression\n",
    "\n",
    "![](ml701.png)\n",
    "\n",
    "2D数据压缩成1D（直角坐标系寻找直线），3D数据压缩成2D（空间坐标系寻找平面）\n",
    "\n",
    "$x^{(1)}∈R^2  ----> \\quad z^{(1)}∈R$\n",
    "\n",
    "$x^{(2)}∈R^2  ----> \\quad z^{(2)}∈R$\n",
    "\n",
    "$x^{(3)}∈R^2  ----> \\quad z^{(3)}∈R$\n",
    "\n",
    "$x^{(m)}∈R^2  ----> \\quad z^{(m)}∈R$\n",
    "\n",
    "###  7.2.2 Data Visualization\n",
    "\n",
    "此外，对于 n = 1000 的高纬度数据，使用聚类可以将数据压缩到2-3维并进行可视化，在某种程度上这非常有用。这可以改变多维数据中难以找到一个有代表性的维度来进行有意义表示数据的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Principal Component Analysis \n",
    "\n",
    "### 7.3.1 Problem formulation\n",
    "\n",
    "PCA 主成分分析主要是将高纬度数据降低维度进行处理的方法。这有以下好处：\n",
    "\n",
    "节省内存和计算性能，加速算法运行。比如将100×100的图片降为成30×30。\n",
    "\n",
    "PCA的主要内容是：对于 n 维降到 k 维而言，找到 k 个 μ 向量（$μ^{(i)}∈R^n$）(对于 2d to 1d 的话，使用 $μ^{(1)}$即可)，使得所有的数据在这个由 k 个向量组成的子空间上的投影的值最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 PCA vs Linear Regression\n",
    "\n",
    "线性回归的拟合使用的是距离并非是数据到向量的垂直距离作为J函数的计算方法。\n",
    "\n",
    "其次，线性回归使用了带有标签的数据，而PCA则这是利用到了自变量。\n",
    "\n",
    "### 7.3.3 PCA algorithm\n",
    "\n",
    "**Data preprocessing**\n",
    "\n",
    "Input: \n",
    "\n",
    "- Training set $x^{(1)},x^{(2)},x^{(3)},...,x^{(m)}$\n",
    "\n",
    "$x^{(i)}∈R^n$ (Drop $x_0 = 1$ converntion)\n",
    "\n",
    "Feature scaling/ mean normalization：\n",
    "\n",
    "$μ_j = \\frac{1}{m} \\sum_{i=1}^m x_j^{(i)}$\n",
    "\n",
    "然后对每个 $x_j$ 使用 $x_j-μ_j$ 进行替换。\n",
    "\n",
    "使用$x_j/标准差或者全距$作为最终输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Algorithm**：\n",
    "\n",
    "1.Compute “covariance martix” 计算协方差矩阵：\n",
    "\n",
    "$\\sum = \\frac{1}{m}(x^{(i)})(x^{(i)})^T \\quad \\sum ∈ R^{n×n}$ \n",
    "\n",
    "2.Compute “eigenvectors” of sigma 求解奇异阵：\n",
    "\n",
    "$[u,s,y] = svd(\\sum) \\quad u ∈ R^{n×n}$\n",
    "\n",
    "3.从 u 中选取前 k 列作为 $u_{reduce}$：\n",
    "\n",
    "$z^{(i)}=u_{reduce} · x^{(i)} \\quad z ∈ R^{n×1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很容易，有了 z 和 x 的关系，那么可以从低维度推测高维度的近似值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing k/数据损失**\n",
    "\n",
    "使用下面公式评估降维的数据损失：\n",
    "\n",
    "$$\\frac{\\frac{1}{m}\\sum_{i=1}^m ||x^{(i)}-x_{approx}^{i}||^2}{\\frac{1}{m}\\sum_{i=1}^m||x^{(i)}||^2} \\le 0.01$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个公式代表99%的数据保留量。\n",
    "\n",
    "一般来说，从 k = 1 开始计算，得到 z、U，x，然后计算数据损失，但那时这样计算量很大，如果有1000个 k，那么要计算1000次。\n",
    "\n",
    "可以使用 $[u,s,y] = svd(\\sum) \\quad u ∈ R^{n×n}$ 中的 s ，\n",
    "\n",
    "$$\\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^mS_{ii}} \\ge 0.99$$\n",
    "\n",
    "对应着99%的数据保留量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，PCA的主要流程就是，从 k = 1 开始进行计算，根据算法求得J和数据损失，直到数据损失达到容忍度，选择此时的 k 值以及此时的 x 的近似值即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.4 PCA Applying Advice\n",
    "\n",
    "- 对于Logistic回归等，可以使用PCA化简数据，加速算法运行，减小内存和硬盘使用。\n",
    "\n",
    "- 可以对高维度数据进行可视化工作。\n",
    "\n",
    "- 不要！不要！不要使用PCA来避免过拟合，因为PCA没有用到标签数据，其作用主要是数据压缩，对过拟合（和标签有关）没有效果。推荐在回归时不要考虑PCA，除非有性能问题再使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "———————————————————————————\n",
    "\n",
    "**更新日志**\n",
    "\n",
    "2018-05-20 SP1.0-180520 添加笔记"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
